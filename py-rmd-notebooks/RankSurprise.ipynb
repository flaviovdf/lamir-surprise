{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf8\n",
    "\n",
    "\n",
    "from scipy.special import gammaln\n",
    "from scipy.special import psi\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_matplotlib():\n",
    "    plt.rcParams['axes.labelsize'] = 12\n",
    "    plt.rcParams['axes.titlesize'] = 12\n",
    "    plt.rcParams['legend.fontsize'] = 12\n",
    "    plt.rcParams['xtick.labelsize'] = 12\n",
    "    plt.rcParams['ytick.labelsize'] = 12\n",
    "    plt.rcParams['lines.linewidth'] = 3\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.style.use('tableau-colorblind10')\n",
    "    \n",
    "def despine(ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # Hide the right and top spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    # Only show ticks on the left and bottom spines\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_matplotlib()\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../py-code')\n",
    "import amutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmusic_json = amutils.load_am_json_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {}\n",
    "for key, val in allmusic_json.items():\n",
    "    names[key] = val['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_centralities(fpath):\n",
    "    df = pd.read_csv(fpath, index_col=0)\n",
    "    new_index = []\n",
    "    if 'allmusic' in fpath:\n",
    "        for id_ in df.index:\n",
    "            new_index.append(names[id_])\n",
    "        df.index = pd.Index(new_index)\n",
    "    \n",
    "    node2id = {}\n",
    "    id2node = {}\n",
    "    for artist_id in df.index:\n",
    "        if type(artist_id) != str and np.isnan(artist_id):\n",
    "            continue\n",
    "        key = artist_id\n",
    "        if key in node2id:\n",
    "            key = '{}-2nd-entry'.format(key)\n",
    "        node2id[artist_id] = len(node2id)\n",
    "        id2node[node2id[artist_id]] = artist_id\n",
    "    \n",
    "    delta = int(df.columns[1]) - int(df.columns[0])\n",
    "    return df.sort_index(), node2id, id2node, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_positions(decade, node2id, df_centrality):\n",
    "    n = len(node2id)\n",
    "    aux = np.zeros(n)\n",
    "    for node, value in df_centrality[str(decade)].iteritems():\n",
    "        if type(node) != str and np.isnan(node):\n",
    "            continue\n",
    "        if np.isnan(value):\n",
    "            aux[node2id[node]] = -np.inf\n",
    "        else:\n",
    "            aux[node2id[node]] = value\n",
    "    \n",
    "    ranks = ss.rankdata(aux, method='max')\n",
    "    \n",
    "    values = np.zeros(shape=(2, n), dtype='d')\n",
    "    values[0] = n - ranks # number above\n",
    "    values[1] = ranks     # number below\n",
    "    \n",
    "    return values.T + 1   # +1 to avoid zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_surprise_detector(observed, prior):\n",
    "    '''\n",
    "    Computes the rank based surprise.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    observed: matrix with the number of nodes with centrality\n",
    "              values greater then (observed[i][0]) or lower\n",
    "              then (observed[i][1]) node i.\n",
    "    prior:    The prior for each node. prior[i][0] is alpha, and\n",
    "              prior[i][1] is beta. In our paper, we set the prior\n",
    "              for each such that prior[i][0] / sum(prior[i])\n",
    "              captures the fraction of other nodes with centrality\n",
    "              greater than node i.\n",
    "    \n",
    "    Both arguments are o shape (n_nodes, 2). Organizing data like\n",
    "    this make's it easy to compute the posterior = observed + prior.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The posterior and the surprise for each node\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    [1]  Penny  WD  (2001):  “KL-Divergences  of  Normal,\n",
    "         Gamma,  Dirichlet  and  Wishartdensities”.\n",
    "         University College, London;\n",
    "         URL: www.fil.ion.ucl.ac.uk/∼wpenny/publications/densities.ps.\n",
    "    [2]  Kullback-Leibler Divergence Between Two Dirichlet\n",
    "         (and Beta) Distributions. URL: http://bariskurt.com/\n",
    "    '''\n",
    "    posterior = prior + observed\n",
    "    prior = (prior.T / prior.sum(axis=1)).T\n",
    "    posterior = (posterior.T / posterior.sum(axis=1)).T\n",
    "    \n",
    "    # from here the code is a dkl divergence of dirichlets. adapted and double\n",
    "    # checked [1] and [2]\n",
    "    \n",
    "    d_obs = gammaln(posterior.sum(axis=1)) - np.sum(gammaln(posterior), axis=1)\n",
    "    d_pri = gammaln(prior.sum(axis=1)) - np.sum(gammaln(prior), axis=1)\n",
    "    individual_factors = (posterior - prior).T * \\\n",
    "            (psi(posterior).T - psi(posterior.sum(axis=1)))\n",
    "    surprises = d_obs - d_pri + individual_factors.sum(axis=0)\n",
    "    \n",
    "    return prior, posterior, surprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_past(year, delta, node2id, df):\n",
    "    return get_rank_positions(year - delta, node2id, df)\n",
    "\n",
    "def hypothesis_growth(year, delta, node2id, df):\n",
    "    position_2d_ago = get_rank_positions(year - 2 * delta, node2id, df)\n",
    "    position_1d_ago = get_rank_positions(year - delta, node2id, df)\n",
    "    prior = position_1d_ago * (position_1d_ago / position_2d_ago)\n",
    "    return prior\n",
    "\n",
    "def hypothesis_flat(year, delta, node2id, df):\n",
    "    prior = get_rank_positions(year, node2id, df) # just to get the shape\n",
    "    prior[:] = 1.0\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, node2id, id2node, delta = load_centralities('../centrality-csvs/allmusic_pageranks.csv')\n",
    "for year in map(int, df.columns[2:]):\n",
    "    obs = get_rank_positions(year, node2id, df)\n",
    "    prior, posterior, surprises = \\\n",
    "            rank_surprise_detector(obs,\n",
    "                                   hypothesis_past(year,\n",
    "                                                   delta,\n",
    "                                                   node2id,\n",
    "                                                   df))\n",
    "    \n",
    "    print(year, ' mean surprise ->', surprises.mean())\n",
    "    top = surprises.argsort()[-5:][::-1]\n",
    "    for ni in top:\n",
    "        p = obs[ni][0] / obs[ni].sum()\n",
    "        beta = ss.beta(a=posterior[ni][0], b=posterior[ni][1])\n",
    "        print(year-delta, year, id2node[ni],\n",
    "              surprises[ni],\n",
    "              beta.pdf(p),\n",
    "              sep='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for decade in map(int, df.columns[-1:]):\n",
    "    print('{} -> {}'.format(decade-delta, decade))\n",
    "        \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 4.5))\n",
    "    \n",
    "    prior = hypothesis_growth(decade, delta, node2id, df)\n",
    "    obs = get_rank_positions(decade, node2id, df)\n",
    "    surprises = rank_surprise_detector(obs, prior)[-1]\n",
    "    \n",
    "    prev = prior[:, 0] / prior.sum(axis=1)\n",
    "    new = obs[:, 0] / obs.sum(axis=1)\n",
    "    \n",
    "    y = new\n",
    "    x = prev\n",
    "    z = surprises\n",
    "    \n",
    "    axes[0].tricontour(x, y, surprises, levels=10, linewidths=0.5, colors='k')\n",
    "    im = axes[0].tricontourf(x, y, surprises, levels=10, cmap='magma_r')\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=axes[0])\n",
    "    cbar.set_label('Surprise')\n",
    "\n",
    "    l = np.linspace(0, min(x.max(), y.max()), 10)\n",
    "    axes[0].plot(l, l, '--', color='magenta', lw=1)\n",
    "    axes[0].set_xlabel(r'$\\hat{\\theta}_{i, t-1}$: Frac. superior on timestamp t-1')\n",
    "    axes[0].set_ylabel(r'$\\hat{\\theta}_{i, t}$: Frac. superior on timestamp t')\n",
    "    despine(axes[0])\n",
    "    \n",
    "    axes[1].tricontour(x, y, np.abs(x-y), levels=10, linewidths=0.5, colors='k')\n",
    "    im = axes[1].tricontourf(x, y, np.abs(x-y), levels=10, cmap='magma_r')    \n",
    "    cbar = fig.colorbar(im, ax=axes[1])\n",
    "    cbar.set_label('Mod Diff.')\n",
    "    \n",
    "    l = np.linspace(0, min(x.max(), y.max()), 10)\n",
    "    axes[1].plot(l, l, '--', color='magenta', lw=1)\n",
    "    axes[1].set_xlabel(r'$\\hat{\\theta}_{i, t-1}$: Frac. superior on timestamp t-1')\n",
    "    axes[1].set_ylabel(r'$\\hat{\\theta}_{i, t}$: Frac. superior on timestamp t')\n",
    "    despine(axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CSVs for Paper from Here\n",
    "\n",
    "From here it's just one big script to generate the csvs used for results in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pagerank, node2id_pagerank, id2node_pagerank, _ = \\\n",
    "        load_centralities('../centrality-csvs/whosampled_pageranks_per_year.csv')\n",
    "df_disrupt, node2id_disrupt, id2node_disrupt, delta = \\\n",
    "        load_centralities('../centrality-csvs/whosampled_disruption_per_year.csv')\n",
    "\n",
    "assert len(df_pagerank) == len(df_disrupt)\n",
    "for i in range(len(df_pagerank)-1): # theres an empty line in the csvs, leads to nans, thus the -1\n",
    "    assert df_pagerank.index[i] == df_disrupt.index[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = {'name' : [],\n",
    "      'year': [],\n",
    "\n",
    "      'pagerank_obs_curr_t_nabove': [],\n",
    "      'pagerank_obs_curr_t_nbelow': [],\n",
    "\n",
    "      'pagerank_prior_hyp_past_t_alpha': [],\n",
    "      'pagerank_prior_hyp_past_t_beta': [],\n",
    "      'pagerank_posterior_hyp_past_t_alpha': [],\n",
    "      'pagerank_posterior_hyp_past_t_beta': [],\n",
    "      'pagerank_surprise_hyp_past_t': [],\n",
    "\n",
    "      'pagerank_prior_hyp_growth_t_alpha': [],\n",
    "      'pagerank_prior_hyp_growth_t_beta': [],\n",
    "      'pagerank_posterior_hyp_growth_t_alpha': [],\n",
    "      'pagerank_posterior_hyp_growth_t_beta': [],\n",
    "      'pagerank_surprise_hyp_growth_t': [],\n",
    "      \n",
    "      'disrupt_obs_curr_t_nabove': [],\n",
    "      'disrupt_obs_curr_t_nbelow': [],\n",
    "      \n",
    "      'disrupt_prior_hyp_past_t_alpha': [],\n",
    "      'disrupt_prior_hyp_past_t_beta': [],\n",
    "      'disrupt_posterior_hyp_past_t_alpha': [],\n",
    "      'disrupt_posterior_hyp_past_t_beta': [],\n",
    "      'disrupt_surprise_hyp_past_t': [],\n",
    "      \n",
    "      'disrupt_prior_hyp_growth_t_alpha': [],\n",
    "      'disrupt_prior_hyp_growth_t_beta': [],\n",
    "      'disrupt_posterior_hyp_growth_t_alpha': [],\n",
    "      'disrupt_posterior_hyp_growth_t_beta': [],\n",
    "      'disrupt_surprise_hyp_growth_t': []\n",
    "     \n",
    "     }\n",
    "\n",
    "for year in map(int, df_pagerank.columns[2:]):\n",
    "    obs_pagerank = get_rank_positions(year, node2id_pagerank, df_pagerank)\n",
    "    rv['name'].extend([id2node[i] for i in range(len(obs_pagerank))])\n",
    "    rv['year'].extend([year for _ in range(len(obs_pagerank))])\n",
    "    \n",
    "    prior_past, posterior_past, surprises_past = \\\n",
    "            rank_surprise_detector(obs_pagerank,\n",
    "                                   hypothesis_past(year,\n",
    "                                                   delta,\n",
    "                                                   node2id_pagerank,\n",
    "                                                   df_pagerank))\n",
    "    \n",
    "    prior_growth, posterior_growth, surprises_growth = \\\n",
    "            rank_surprise_detector(obs_pagerank,\n",
    "                                   hypothesis_past(year,\n",
    "                                                   delta,\n",
    "                                                   node2id_pagerank,\n",
    "                                                   df_pagerank))\n",
    "    \n",
    "    rv['pagerank_obs_curr_t_nabove'].extend(obs_pagerank[:, 0])\n",
    "    rv['pagerank_obs_curr_t_nbelow'].extend(obs_pagerank[:, 1])\n",
    "    \n",
    "    rv['pagerank_prior_hyp_past_t_alpha'].extend(prior_past[:, 0])\n",
    "    rv['pagerank_prior_hyp_past_t_beta'].extend(prior_past[:, 1])\n",
    "    rv['pagerank_posterior_hyp_past_t_alpha'].extend(posterior_past[:, 0])\n",
    "    rv['pagerank_posterior_hyp_past_t_beta'].extend(posterior_past[:, 1])\n",
    "    rv['pagerank_surprise_hyp_past_t'].extend(surprises_past)\n",
    "    \n",
    "    rv['pagerank_prior_hyp_growth_t_alpha'].extend(prior_growth[:, 0])\n",
    "    rv['pagerank_prior_hyp_growth_t_beta'].extend(prior_growth[:, 1])\n",
    "    rv['pagerank_posterior_hyp_growth_t_alpha'].extend(posterior_growth[:, 0])\n",
    "    rv['pagerank_posterior_hyp_growth_t_beta'].extend(posterior_growth[:, 1])\n",
    "    rv['pagerank_surprise_hyp_growth_t'].extend(surprises_growth)\n",
    "    \n",
    "    \n",
    "    obs_disrupt = get_rank_positions(year, node2id_disrupt, df_disrupt)\n",
    "    prior_past, posterior_past, surprises_past = \\\n",
    "            rank_surprise_detector(obs_disrupt,\n",
    "                                   hypothesis_past(year,\n",
    "                                                   delta,\n",
    "                                                   node2id_disrupt,\n",
    "                                                   df_disrupt))\n",
    "    \n",
    "    prior_growth, posterior_growth, surprises_growth = \\\n",
    "            rank_surprise_detector(obs_disrupt,\n",
    "                                   hypothesis_past(year,\n",
    "                                                   delta,\n",
    "                                                   node2id_disrupt,\n",
    "                                                   df_disrupt))\n",
    "    \n",
    "    rv['disrupt_obs_curr_t_nabove'].extend(obs_disrupt[:, 0])\n",
    "    rv['disrupt_obs_curr_t_nbelow'].extend(obs_disrupt[:, 1])\n",
    "    \n",
    "    rv['disrupt_prior_hyp_past_t_alpha'].extend(prior_past[:, 0])\n",
    "    rv['disrupt_prior_hyp_past_t_beta'].extend(prior_past[:, 1])\n",
    "    rv['disrupt_posterior_hyp_past_t_alpha'].extend(posterior_past[:, 0])\n",
    "    rv['disrupt_posterior_hyp_past_t_beta'].extend(posterior_past[:, 1])\n",
    "    rv['disrupt_surprise_hyp_past_t'].extend(surprises_past)\n",
    "    \n",
    "    rv['disrupt_prior_hyp_growth_t_alpha'].extend(prior_growth[:, 0])\n",
    "    rv['disrupt_prior_hyp_growth_t_beta'].extend(prior_growth[:, 1])\n",
    "    rv['disrupt_posterior_hyp_growth_t_alpha'].extend(posterior_growth[:, 0])\n",
    "    rv['disrupt_posterior_hyp_growth_t_beta'].extend(posterior_growth[:, 1])\n",
    "    rv['disrupt_surprise_hyp_growth_t'].extend(surprises_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = pd.DataFrame(rv)\n",
    "rv.to_csv('whosampled_surprises.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
